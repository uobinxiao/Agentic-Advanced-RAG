{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset from NarrativeQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Narrative from Question Generation and RAG Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/poetry310/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetLoads initialized\n"
     ]
    }
   ],
   "source": [
    "from Utils import *\n",
    "data_loader = DatasetLoader()\n",
    "from datasets import load_dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "def extract_narrativeqa_text(split='train'):\n",
    "    # Load the dataset\n",
    "    dataset = load_dataset(\"deepmind/narrativeqa\", split=split)\n",
    "    \n",
    "    # Dictionaries to store unique texts\n",
    "    unique_summaries = {}\n",
    "    unique_documents = {}\n",
    "    \n",
    "    total_summary_chars = 0\n",
    "    total_document_chars = 0\n",
    "    \n",
    "    # Extract text from each example\n",
    "    for example in dataset:\n",
    "        summary = example['document']['summary']['text']\n",
    "        document = example['document']['text']\n",
    "        metadata = example['document']['kind']\n",
    "        \n",
    "        # Only add if both summary and document are unique\n",
    "        if summary not in unique_summaries and document not in unique_documents:\n",
    "            unique_summaries[summary] = metadata\n",
    "            unique_documents[document] = metadata\n",
    "            total_summary_chars += len(summary)\n",
    "            total_document_chars += len(document)\n",
    "    \n",
    "    # Create lists from the dictionaries\n",
    "    summaries = list(unique_summaries.keys())\n",
    "    documents = list(unique_documents.keys())\n",
    "    metadata = [unique_summaries[s] for s in summaries]  # align metadata with summaries\n",
    "    \n",
    "    # Calculate averages\n",
    "    num_examples = len(summaries)\n",
    "    avg_summary_chars = total_summary_chars / num_examples if num_examples > 0 else 0\n",
    "    avg_document_chars = total_document_chars / num_examples if num_examples > 0 else 0\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'summary': summaries,\n",
    "        'document': documents,\n",
    "        'metadata': metadata\n",
    "    })\n",
    "    \n",
    "    print(f'Number of unique examples: {num_examples}')\n",
    "    print(f'Average summary length: {avg_summary_chars:.2f} characters')\n",
    "    print(f'Average document length: {avg_document_chars:.2f} characters')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "# df = extract_narrativeqa_text(split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique examples: 1102\n",
      "Average summary length: 3392.44 characters\n",
      "Average document length: 343771.38 characters\n",
      "1102\n",
      "5098 798807\n"
     ]
    }
   ],
   "source": [
    "df_text = extract_narrativeqa_text(split=\"train\")\n",
    "print(len(df_text))\n",
    "print(len(df_text[\"summary\"][0]), len(df_text[\"document\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sample_text = df_text.sample(frac=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>document</th>\n",
       "      <th>metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>The narrative begins with the formation of th...</td>\n",
       "      <td>ï»¿The Project Gutenberg EBook of The American...</td>\n",
       "      <td>gutenberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Four days after the events of Rush Hour, LAPD...</td>\n",
       "      <td>&lt;html&gt;\\n&lt;head&gt;&lt;title&gt;Rush Hour 2 Script at IMS...</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>The first story, \"The Blonde Lady\", opens wit...</td>\n",
       "      <td>ï»¿Project Gutenberg's ArsÃ¨ne Lupin versus He...</td>\n",
       "      <td>gutenberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>Junior risk analyst Seth Bregman (Penn Badgle...</td>\n",
       "      <td>&lt;html&gt;\\n&lt;head&gt;&lt;title&gt;Margin Call Script at IMS...</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>The film takes place in 1936, at the height o...</td>\n",
       "      <td>&lt;html&gt;\\n&lt;head&gt;\\n&lt;script&gt;\\n&lt;!--\\n\\n/*\\nBreak-ou...</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                summary  \\\n",
       "1095   The narrative begins with the formation of th...   \n",
       "99     Four days after the events of Rush Hour, LAPD...   \n",
       "358    The first story, \"The Blonde Lady\", opens wit...   \n",
       "320    Junior risk analyst Seth Bregman (Penn Badgle...   \n",
       "550    The film takes place in 1936, at the height o...   \n",
       "\n",
       "                                               document   metadata  \n",
       "1095  ï»¿The Project Gutenberg EBook of The American...  gutenberg  \n",
       "99    <html>\\n<head><title>Rush Hour 2 Script at IMS...      movie  \n",
       "358   ï»¿Project Gutenberg's ArsÃ¨ne Lupin versus He...  gutenberg  \n",
       "320   <html>\\n<head><title>Margin Call Script at IMS...      movie  \n",
       "550   <html>\\n<head>\\n<script>\\n<!--\\n\\n/*\\nBreak-ou...      movie  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_sample_text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(random_sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform the dataframe into .txts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_text_to_files_by_metadata(df):\n",
    "    # Ensure the dataframe has the required columns\n",
    "    if not all(col in df.columns for col in ['document', 'metadata']):\n",
    "        raise ValueError(\"Dataframe must contain 'document' and 'metadata' columns\")\n",
    "\n",
    "    # Dictionary to keep track of file handles\n",
    "    file_handles = {}\n",
    "\n",
    "    try:\n",
    "        for _, row in df.iterrows():\n",
    "            metadata = row['metadata']\n",
    "            document = row['document']\n",
    "\n",
    "            # Create or get file handle\n",
    "            if metadata not in file_handles:\n",
    "                filename = f\".txt/{metadata}.txt\"\n",
    "                file_handles[metadata] = open(filename, 'a', encoding='utf-8')\n",
    "\n",
    "            # Write document to file\n",
    "            file_handles[metadata].write(document + \"\\n\\n\")  # Add two newlines for separation\n",
    "\n",
    "    finally:\n",
    "        # Close all file handles\n",
    "        for handle in file_handles.values():\n",
    "            handle.close()\n",
    "\n",
    "    print(f\"Files created: {', '.join(f'{metadata}.txt' for metadata in file_handles.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files created: gutenberg.txt, movie.txt\n"
     ]
    }
   ],
   "source": [
    "write_text_to_files_by_metadata(random_sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedder into Milvus (GPU) for normal dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Milvus at localhost:19530 with database default.\n",
      "VectorDatabase initialized.\n",
      "Initializing sparse embedder...\n",
      "Embedder initialized\n",
      "Data Processor initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/yarikama/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "vector_DB = VectorDatabase()\n",
    "embedder = Embedder()\n",
    "data_processor = DataProcessor(embedder=embedder, vectordatabase=vector_DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.document import Document\n",
    "\n",
    "def transform_to_langchain_documents(df):\n",
    "    \"\"\"\n",
    "    Transform the DataFrame into a list of Langchain Document objects.\n",
    "    \n",
    "    Args:\n",
    "    df (pandas.DataFrame): DataFrame with 'document', 'summary', and 'metadata' columns.\n",
    "    \n",
    "    Returns:\n",
    "    list: A list of Langchain Document objects.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    for _, row in df.iterrows():\n",
    "        doc = Document(\n",
    "            page_content=row['document'],\n",
    "            metadata={\n",
    "                \"kind\": row['metadata'],\n",
    "            }\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Document\nmetadata\n  value is not a valid dict (type=type_error.dict)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_to_langchain_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_sample_text\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 15\u001b[0m, in \u001b[0;36mtransform_to_langchain_documents\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     13\u001b[0m documents \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m---> 15\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mDocument\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpage_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdocument\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     documents\u001b[38;5;241m.\u001b[39mappend(doc)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m documents\n",
      "File \u001b[0;32m/opt/anaconda3/envs/poetry310/lib/python3.10/site-packages/langchain_core/documents/base.py:270\u001b[0m, in \u001b[0;36mDocument.__init__\u001b[0;34m(self, page_content, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Pass page_content in as positional or named arg.\"\"\"\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# my-py is complaining that page_content is not defined on the base class.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# Here, we're relying on pydantic base class to handle the validation.\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpage_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpage_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/poetry310/lib/python3.10/site-packages/langchain_core/load/serializable.py:113\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/poetry310/lib/python3.10/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for Document\nmetadata\n  value is not a valid dict (type=type_error.dict)"
     ]
    }
   ],
   "source": [
    "docs = transform_to_langchain_documents(random_sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedder into Milvus (GPU) for txts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor.directory_files_process(\"routing_narrativeqa\", \".txt/\", True, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poetry310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
