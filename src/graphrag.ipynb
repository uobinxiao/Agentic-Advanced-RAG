{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset from NarrativeQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Narrative from Question Generation and RAG Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetLoads initialized\n",
      "Connected to Milvus at localhost:19530 with database default.\n",
      "VectorDatabase initialized.\n",
      "Initializing sparse embedder...\n",
      "Embedder initialized\n",
      "Data Processor initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/yarikama/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from Utils import *\n",
    "data_loader = DatasetLoader()\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "vector_DB = VectorDatabase()\n",
    "embedder = Embedder()\n",
    "data_processor = DataProcessor(embedder=embedder, vectordatabase=vector_DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_narrativeqa_text(split='train'):\n",
    "    # Load the dataset\n",
    "    dataset = load_dataset(\"deepmind/narrativeqa\", split=split)\n",
    "    \n",
    "    # Dictionaries to store unique texts\n",
    "    unique_summaries = {}\n",
    "    unique_documents = {}\n",
    "    \n",
    "    total_summary_chars = 0\n",
    "    total_document_chars = 0\n",
    "    \n",
    "    # Extract text from each example\n",
    "    for example in dataset:\n",
    "        summary = example['document']['summary']['text']\n",
    "        document = example['document']['text']\n",
    "        metadata = example['document']['kind']\n",
    "        \n",
    "        # Only add if both summary and document are unique\n",
    "        if summary not in unique_summaries and document not in unique_documents:\n",
    "            unique_summaries[summary] = metadata\n",
    "            unique_documents[document] = metadata\n",
    "            total_summary_chars += len(summary)\n",
    "            total_document_chars += len(document)\n",
    "    \n",
    "    # Create lists from the dictionaries\n",
    "    summaries = list(unique_summaries.keys())\n",
    "    documents = list(unique_documents.keys())\n",
    "    metadata = [unique_summaries[s] for s in summaries]  # align metadata with summaries\n",
    "    \n",
    "    # Calculate averages\n",
    "    num_examples = len(summaries)\n",
    "    avg_summary_chars = total_summary_chars / num_examples if num_examples > 0 else 0\n",
    "    avg_document_chars = total_document_chars / num_examples if num_examples > 0 else 0\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'summary': summaries,\n",
    "        'document': documents,\n",
    "        'metadata': metadata\n",
    "    })\n",
    "    \n",
    "    print(f'Number of unique examples: {num_examples}')\n",
    "    print(f'Average summary length: {avg_summary_chars:.2f} characters')\n",
    "    print(f'Average document length: {avg_document_chars:.2f} characters')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "# df = extract_narrativeqa_text(split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd746bd7ae0c40be863df41b5b2288a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b107381447004c809d1e6618aeafb829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59dcba8f1807447b992edfa34bd87ea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique examples: 1102\n",
      "Average summary length: 3392.44 characters\n",
      "Average document length: 343771.38 characters\n",
      "1102\n",
      "5098 798807\n"
     ]
    }
   ],
   "source": [
    "df_text = extract_narrativeqa_text(split=\"train\")\n",
    "print(len(df_text))\n",
    "print(len(df_text[\"summary\"][0]), len(df_text[\"document\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1102"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sample_text = df_text.sample(frac=0.01)\n",
    "# random_sample_text.to_parquet(\".parquet/narrative_qa_sample_11.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>document</th>\n",
       "      <th>metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Crash\" Davis (Costner), a veteran of 12 year...</td>\n",
       "      <td>&lt;html&gt;\\n&lt;head&gt;&lt;title&gt;Bull Durham Script at IMS...</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>The Vicar - Dr Charles Primrose - lives an id...</td>\n",
       "      <td>ï»¿The Project Gutenberg EBook of The Vicar of...</td>\n",
       "      <td>gutenberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>At a bistro in the Montmartre district of Par...</td>\n",
       "      <td>&lt;html&gt;\\n&lt;head&gt;&lt;title&gt;Ronin Script at IMSDb.&lt;/t...</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>According to The Oxford Companion to English ...</td>\n",
       "      <td>ï»¿The Project Gutenberg EBook of Adam Bede, b...</td>\n",
       "      <td>gutenberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>In the woods outside of Cherry Falls, Virgini...</td>\n",
       "      <td>&lt;html&gt;\\n&lt;head&gt;&lt;title&gt;Cherry Falls Script at IM...</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>Tom Swift's father has been working diligentl...</td>\n",
       "      <td>ï»¿Project Gutenberg's Tom Swift and his Subma...</td>\n",
       "      <td>gutenberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>The novel, which is intensely autobiographica...</td>\n",
       "      <td>ï»¿The Project Gutenberg EBook of The Book of ...</td>\n",
       "      <td>gutenberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>In the winter of 1987, Minneapolis car salesm...</td>\n",
       "      <td>&lt;html&gt;\\n&lt;head&gt;&lt;title&gt;Fargo Script at IMSDb.&lt;/t...</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>A wealthy American man named Longmore is intr...</td>\n",
       "      <td>ï»¿The Project Gutenberg EBook of Madame de Ma...</td>\n",
       "      <td>gutenberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>Trevor Gooden (Dean Winters) survives a car a...</td>\n",
       "      <td>&lt;html&gt;\\n&lt;head&gt;&lt;title&gt;Hellraiser: Hellseeker Sc...</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>Set in Ewedown, a fictitious village in Dorse...</td>\n",
       "      <td>&lt;html&gt;\\n&lt;head&gt;&lt;title&gt;Tamara Drewe Script at IM...</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               summary  \\\n",
       "4     \"Crash\" Davis (Costner), a veteran of 12 year...   \n",
       "156   The Vicar - Dr Charles Primrose - lives an id...   \n",
       "805   At a bistro in the Montmartre district of Par...   \n",
       "492   According to The Oxford Companion to English ...   \n",
       "713   In the woods outside of Cherry Falls, Virgini...   \n",
       "599   Tom Swift's father has been working diligentl...   \n",
       "318   The novel, which is intensely autobiographica...   \n",
       "19    In the winter of 1987, Minneapolis car salesm...   \n",
       "617   A wealthy American man named Longmore is intr...   \n",
       "472   Trevor Gooden (Dean Winters) survives a car a...   \n",
       "639   Set in Ewedown, a fictitious village in Dorse...   \n",
       "\n",
       "                                              document   metadata  \n",
       "4    <html>\\n<head><title>Bull Durham Script at IMS...      movie  \n",
       "156  ï»¿The Project Gutenberg EBook of The Vicar of...  gutenberg  \n",
       "805  <html>\\n<head><title>Ronin Script at IMSDb.</t...      movie  \n",
       "492  ï»¿The Project Gutenberg EBook of Adam Bede, b...  gutenberg  \n",
       "713  <html>\\n<head><title>Cherry Falls Script at IM...      movie  \n",
       "599  ï»¿Project Gutenberg's Tom Swift and his Subma...  gutenberg  \n",
       "318  ï»¿The Project Gutenberg EBook of The Book of ...  gutenberg  \n",
       "19   <html>\\n<head><title>Fargo Script at IMSDb.</t...      movie  \n",
       "617  ï»¿The Project Gutenberg EBook of Madame de Ma...  gutenberg  \n",
       "472  <html>\\n<head><title>Hellraiser: Hellseeker Sc...      movie  \n",
       "639  <html>\\n<head><title>Tamara Drewe Script at IM...      movie  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load parquet\n",
    "samples = pd.read_parquet(\".parquet/narrative_qa_sample_11.parquet\")\n",
    "samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform the dataframe into .txts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "\n",
    "def preprocess_content(content: str) -> str:\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "\n",
    "    # 統一為 NFKC 正規化形式\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "\n",
    "    # 移除 URL\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "\n",
    "    # 移除多餘的空白字符\n",
    "    text = re.sub(r'[ \\t]+', ' ', text).strip()\n",
    "\n",
    "    # 移除特殊字符，但保留某些標點符號\n",
    "    text = re.sub(r'[^\\w\\s.,!?;:()\"-]', '', text)\n",
    "\n",
    "    # 統一引號\n",
    "    text = text.replace('\"', '\"').replace('\"', '\"')\n",
    "\n",
    "    # 移除連續的標點符號\n",
    "    text = re.sub(r'([.,!?;:])\\1+', r'\\1', text)\n",
    "\n",
    "    # 確保句子之間有適當的空格\n",
    "    text = re.sub(r'([.,!?;:])\\s*', r'\\1 ', text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "def write_text_to_files_by_metadata(df):\n",
    "    # Ensure the dataframe has the required columns\n",
    "    if not all(col in df.columns for col in ['document', 'metadata']):\n",
    "        raise ValueError(\"Dataframe must contain 'document' and 'metadata' columns\")\n",
    "\n",
    "    # Dictionary to keep track of file handles\n",
    "    file_handles = {}\n",
    "\n",
    "    try:\n",
    "        for _, row in df.iterrows():\n",
    "            metadata = row['metadata']\n",
    "            document = preprocess_content(row['document'])\n",
    "\n",
    "            # Create or get file handle\n",
    "            if metadata not in file_handles:\n",
    "                filename = f\".txt/{metadata}.txt\"\n",
    "                file_handles[metadata] = open(filename, 'a', encoding='utf-8')\n",
    "\n",
    "            # Write document to file\n",
    "            file_handles[metadata].write(document + \"\\n\\n\")  # Add two newlines for separation\n",
    "\n",
    "    finally:\n",
    "        # Close all file handles\n",
    "        for handle in file_handles.values():\n",
    "            handle.close()\n",
    "\n",
    "    print(f\"Files created: {', '.join(f'{metadata}.txt' for metadata in file_handles.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files created: movie.txt, gutenberg.txt\n"
     ]
    }
   ],
   "source": [
    "write_text_to_files_by_metadata(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedder into Milvus (GPU) for normal dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.document import Document\n",
    "\n",
    "def transform_to_langchain_documents(df):\n",
    "    \"\"\"\n",
    "    Transform the DataFrame into a list of Langchain Document objects.\n",
    "    \n",
    "    Args:\n",
    "    df (pandas.DataFrame): DataFrame with 'document', 'summary', and 'metadata' columns.\n",
    "    \n",
    "    Returns:\n",
    "    list: A list of Langchain Document objects.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    for _, row in df.iterrows():\n",
    "        doc = Document(\n",
    "            page_content=row['document'],\n",
    "            metadata={\n",
    "                \"kind\": row['metadata'],\n",
    "            }\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = transform_to_langchain_documents(random_sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedder into Milvus (GPU) for txts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor.directory_files_process(\"narrative_qa_standard_gpu\", \".txt/\", True, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poetry310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
